<!DOCTYPE html>
<html ng-app="gzipApp">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <!-- TODO(mankoff): add in responsive design -->
    <style>
      .container { margin: auto; }
      figure { -webkit-margin-start: 0; -webkit-margin-end: 0; }
      figure .visual { display: block; }
      figcaption { font-size: 12px; margin: 0 20px 10px; line-height: 15px; }
      p { font-family: "Times New Roman", serif; font-size: 16px; line-height: 22px; margin-bottom: 18px; }
      .tcp-controls label { display: inline-block; margin-right: 18px; font-size: 12px; }
      .tcp-controls input { width: 50px; display: block; }
      .tcp-controls input[name=num_bytes] { width:90px; }
      a.reference { vertical-align: super; font-size: 12px; }
      svg.chart { font: 10px arial; }
      g.legend rect, g.bar rect { fill: rgb(127, 133, 182); shape-rendering: crispEdges; }
      g.bar text { fill: black;  }
      g.legend g.mobile rect, g.data-set.mobile g.bar rect { fill: rgb(255, 183, 52); }
      .axis path, .axis line { fill: none; stroke: black; shape-rendering: crispEdges; }
      .axis-label { text-anchor: middle; }
      .line { fill: none; stroke-width: 2px; }
      .line.cw { stroke: rgb(127, 133, 182); }
      .line.packet { stroke: rgb(255, 178, 13); }
      .area { stroke: none; }
      .area.compressed { fill: rgb(199, 255, 199); }
      .area.decompressed { fill: rgb(255, 208, 208); }
      .area-label.gzipped { text-anchor: end; }
      .area-label.decompressed { text-anchor: start; }
      .average-savings { text-anchor: end; font-size: 16px; baseline-shift: -16px; }
    </style>
  </head>
<body ng-controller="GzipCtrl">
  <div class="container">
    <div class="row">
      <div class="span9 offset1">
        <h1>Quantifying gzip's Impact on Web Performance</h1>
        <h2>Introduction</h2>
        <p>
          On the Google's PageSpeed mailing list, there was recently a 
          <a href="https://groups.google.com/d/msg/page-speed-discuss/1cRCzYwrc0E/XKimQ4dOupMJ">
            question raised regarding the benefits of gzip</a>. Specifically,
          the rightly curious individual wanted specific numbers regarding the 
          costs of associated with compressed content. While I responded 
          confidently that gzip would improve page performance, I discovered 
          that I could not easily find any recent information to back up my 
          claim.
        </p>
        <p>
          And thus, I did the only logical thing: I collected the data myself. I 
          downloaded CSV of <a href="http://www.alexa.com/topsites">Alexa's top
            one million</a> sites and plugged them into a script that downloaded
          and decompressed each page, timing the decompression as it went. What 
          follows is an explanation of the results and some spiffy interactive 
          graphs to match.
        </p>
        <p>
          For the impatient, you can skip straight to the conclusion: 
          <a href="#conclusion">gzip is a very good thing</a>.
        </p>
        <h2>gzip's Traditional Savings: Bytes on the Wire</h2>
        <p>
          Usually, when people think of "zipping" up their content, they're 
          considering how much smaller the content becomes as a result of 
          compression. If you can compress a 100MB file into a 1MB file, it 
          should download 100 times as quickly, or so goes the thinking.
        </p>
        <figure class="thumbnail">
          <compression-ratio-visual class="visual" data="data_desktop" animation-duration="animation_duration" width="690" height="200"></compression-ratio-visual>
          <figcaption>
            Most sampled pages compresses to between 15% and 35% of its original
            size. The average page is compressed to 24.9% of its original size.
          </figcaption>
        </figure>
        <p>
          Looking at the histogram for the homepages of our sample sites, we can 
          see most pages are compressed to between 15% and 35% of their original 
          size. That's pretty good. For all of the sampled pages, that 
          corresponds to roughly 65MB of data saved.
        </p>
        <p>
          But already, this thinking is flawed. For instance, the advertised
          level of bandwidth offered at one of my local ISPs is 50Mbps. That's 
          close to 6 megabytes per second. If it were that simple, then we could
          expect to download the largest sampled page, sina.com.cn, 
          <em>uncompressed</em> in roughly a tenth of a second. The average
          page, clocking in at 101KB, would take closer to a hundredth of a 
          second. And yet, we know from experience that this is not the case.
          Why?
        </p>
        <h2>Enter TCP Slow-start</h2>
        <p>
          The villain in this case is something known as 
          <a href="http://en.wikipedia.org/wiki/Slow-start">TCP Slow-start</a>.
          I use the term "villain" lightly here because it is actually a very
          useful mechanism for general internet traffic. It prevents competing
          streams of data from stomping all over each other and mucking up the
          internet for everyone. The problem is that slow-start's costs, in terms
          of wall-clock time, is quite high for small data payloads such as HTML 
          and other web content relative to their size.
        </p>
        <p>
          The basic mechanism for slow start goes as such: send a small amount
          of data and wait for it to transfer. If that goes smoothly, send a little
          more data than before. Keep doing this until either everything has been
          sent or data starts getting lost. When data is lost, back up a little,
          and restart the process, starting with the data that was lost. This 
          generally works well when one needs to send a large amount of data: you
          ramp up the amount of data that you're sending and then hover on or 
          around the maximum sending size until you've sent all of your data. When
          working with large amounts of data, this ramp up occurs quickly. The few
          extra round trips that are incurred at the beginning are inconsequential 
          relative to the total send time for the payload.
        </p>
        <p>
          And yet, for small data payloads, much of this extra caution is 
          expensive. Most computer systems will start out only sending 2, 3 or 
          10 "packets" of data before pausing. For reference, 10 packets is 
          approximately 14KB of data. Thus, a 15KB file that fits into 11 
          packets is broken up into 2 separate chunks of packets. The second 
          chunk has to wait on the first to finish before it can be sent. Even 
          on a fast connection, it can take 20ms or so for all the packets in 
          the first chunk to be "ACK'ed". Suddenly, a tiny 15KB file that should
          transfer in 2ms on our super-fast data connection is taking ten times 
          as long. And 20ms is on the optimistic side; it is common to see RTT's 
          of 30 to 60ms. For international communication, you can have RTTs in 
          the hundreds. (A quick test of baidu.com gives me a 600ms average!)
        </p>
        <p>
          These "chunks" as I have referred to them are what's known as a 
          "congestion window", CWND for short. Whereas a packet of data 
          represents a set of bits, a CWND represents a set of packets. 
          Slow-start is the mechanism that controls the size and growth of the 
          CWND. As referenced before, the initial congestion window (ICWND) 
          starts small and grows over time.
        </p>
        <p>
          A full overview of how TCP slow-start works would require an article 
          in and of itself. My colleague Ilya Grigorik has
          <a href="http://www.igvita.com/2011/10/20/faster-web-vs-tcp-slow-start/">
            an article that digs into slow-start and discusses how to deal
            with it</a>. Maybe some day I'll write a more in depth article on
            slow-start's finer points, but for now enough with the words, let's 
            see some pictures!
        </p>
        <h2>gzip to the Rescue</h2>
        <figure class="thumbnail">
          <icw-visual data="data_desktop" max-cw="max_cw" icw="icw" mtu="mtu" ssthresh="ssthresh" rtt="rtt"  congestion-algorithm="congestion_algorithm" num_bytes="max_bytes" compression-ratio="compression_ratio" animation-duration="animation_duration" width="690" height="200"></icw-visual>
          <figcaption>
            A visualization of the congestion window and download time for the
            largest page sampled. The area in green represents the time to 
            download the gzipped page. The area in red represents the additional
            amount of time the non-gzipped page would have taken. Use the 
            contols below to see how different parameters affect the TCP
            pattern.
          </figcaption>
        </figure>
        <form name="tcp_controls" class="tcp-controls well">
          <label>
            ICWND (packets):
            <input name="icw" type="number" ng-model="icw" min="1" ng-pattern="/^\d+$/" />
          </label>
          <label>
            Max CWND (packets):
            <input name="maxcw" type="number" ng-model="max_cw" min="1" ng-pattern="/^\d+$/" />
          </label>
          <label>
            MTU (bytes):
            <input name="mtu" type="number" ng-model="mtu" min="1" ng-pattern="/^\d+$/" />
          </label>
          <label>
            RTT (ms):
            <input name="rtt" type="number" ng-model="rtt" min="1" ng-pattern="/^\d+$/" />
          </label>
          <label>
            Total Bytes:
            <input name="num_bytes" type="number" ng-model="max_bytes" min="1" ng-pattern="/^\d+$/" />
          </label>
          <label>
            Compression:
            <input name="compression_ratio" type="number" ng-model="compression_ratio" min=".01" max="1" step=".01" ng-pattern="/^(0?\.\d+)|1$/" />
          </label>
          <div>
           Congestion Algorithm<a href="http://en.wikipedia.org/wiki/TCP_congestion_avoidance_algorithm#TCP_Tahoe_and_Reno" class="reference" target="_blank">?</a>:
           <div class="btn-group" data-toggle="buttons-radio">
             <button class="btn btn-small" btn-radio="'tahoe'" ng-model="congestion_algorithm">Tahoe</button>
             <button class="btn btn-small" btn-radio="'reno'" ng-model="congestion_algorithm">Reno</button>
             <button class="btn btn-small" btn-radio="'cubic'" ng-model="congestion_algorithm">CUBIC</button>
           </div>
          </div>
        </form>
        <figure class="thumbnail">
          <seconds-saved-visual data="data_desktop" max-cw="max_cw" icw="icw" mtu="mtu" ssthresh="ssthresh" rtt="rtt" congestion-algorithm="congestion_algorithm" animation-duration="animation_duration" width="690" height="200"></seconds-saved-visual>
          <figcaption>
            A histogram showing the number of round trips saved by gzip for the 
            sites sampled. The number at the top shows the number of sites that
            saved the given number of round trips. Use the controls above to see
            how different parameters can affect the round trips saved.
          </figcaption>
        </figure>
        <p>
          The two visuals above should help convey the round trip costs 
          associated with TCP slow-start as well as the savings that gzip
          provides. The first graph shows how the largest site I found loads,
          both with and without gzip turned on. By fitting the compressing the 
          data and fitting it into fewer packets, we reduce the number of round 
          trips that occur and thus avoid the associated latency costs. The 
          second histogram demonstrates the estimated number of round-trips 
          saved for all of the sampled sites.
        </p>
        <p>
          Notice that there is a statistically significant number of sites for
          which gzip <em>does not</em> appear save any round trips. There are a
          few things to keep in mind before jumping to conclusions about this
          apparent anomaly.
        </p>
        <p>
          First, some of the sites scraped returned exceptionally small pages
          and should be excluded. Some sites, for instance, returned error pages
          because the scaper was not logged in or did not have the right cookies
          set. Still others simply don't have a proper home page because they 
          are merely CDN's or other non-public facing pages. Pages such as these
          add noise to the data. In light of this fact, we would actually expect
          the savings coming from gzip to improve if a more comprehensive sample
          of the web was taken.
        </p>
        <p>
          Second, even for the rare page where compression doesn't explicitly 
          save a round trip, there are other benefits. Reducing the number of
          packets sent reduces the risk of encountering packet loss - a very
          expensive penalty in terms of the time it takes to recover. Also, 
          while it might not save round trips, it will save bandwidth. ISPs
          don't charge per CWND, they charge per byte. If you can cut your bytes
          in half, you cut your bandwidth costs in half.
        </p>
        <p>
          Third, and most importantly, just because a page was small for this 
          scrape doesn't mean that it will always be small, especially for 
          dynamically generated pages that can change in size from one request 
          to the next. Nor does it mean that every other page on a site is small
          (I only sampled homepages). gzip is usually turned on as an all or
          nothing flag for text content. There's little sense in turning it on 
          selectively for some pages and not for others.
        </p>
        <p>
          Lastly, do not forget that what is shown above is a very optimistic
          model. Real world results for TCP behavior may vary wildly, both from 
          server to server and connection to connection. There are 
          <a href="http://en.wikipedia.org/wiki/TCP_congestion_avoidance_algorithm">
            many algorithms that affect TCP slow-start</a> and each will change
          the end result.
        </p>
        <figure class="thumbnail">
          <decompress-time-visual data-data-desktop="data_desktop" data-data-mobile="data_mobile" animation-duration="animation_duration" width="690" height="200"></decompress-time-visual>
          <figcaption>
            A histogram showing the average decompression time for the sites 
            sampled in milliseconds. On desktop, most pages take between 0.1 and
            0.7 milliseconds. Mobile is slower in the absolute sense, with most
            pages completing closer to 2 milliseconds, but the overall savings
            in terms of round trips is also significantly higher. In other 
            words, gzip is very fast!
          </figcaption>
        </figure>
================================================================================
        <p>
          And this brings up back full circle to the <em>costs</em> of using 
          gzip. As the figure above shows, gzip is cheap. It is generally two 
          orders of magnitude faster than even a single round trip time. Thus,
          if compressing your content might possibly save you even a single
          round trip, it will easily be worth the nominal expense.
        </p>

        <h2 id="conclusion">Conclusion</h2>
        <p>gzip helps. A lot. Use it.</p>
        <p>
          Of the sites sampled, one can expect between one and three round trips
          to be saved. On mobile devices, decompression takes less than 4ms in 
          the majority of cases and on these devices, where you can easily 
          expect round trip times of 200ms or more, that can mean a savings in 
          upwards of 600ms. That's a very impactful chunk of time.
        </p>
        <p>
          Even on desktops, where RTTs can get closer to 20ms on a good day, you
          can expect every saved round-trip to count. Decompression on desktops
          is blazingly fast &mdash; virtually free when compared against the 
          other bottlenecks involved. You can easily expect to save 100ms or 
          more in common cases.
        </p>
      </div>
    </div>
  </div>

  <script src="js/angular.min.js"></script>
  <script src="js/d3.v3.js" charset="utf-8"></script>
  <script src="js/ui-bootstrap-tpls-0.3.0.min.js"></script>
  <script src="js/main.js"></script>
</body>
